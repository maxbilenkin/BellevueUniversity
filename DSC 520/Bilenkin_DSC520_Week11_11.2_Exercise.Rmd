---
title: "Machine Learning Exercise 11.2"
author: "Maxim Bilenkin"
date: "2025-02-19"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "markup")
```

1.
   e.
   
   i. Plot the data from each dataset using a scatter plot.

Loading libraries and datasets.

```{r, echo=TRUE, results='markup'}
# Loading the necessary libraries and both datasets.
library(readr)
library(class)
library(ggplot2)
library(caret)

# Setting the working directory
setwd("C:/Users/maxim/OneDrive/Desktop/BU/DSC 520")

# Loading datasets
binary_classifier_data <- read_csv("binary-classifier-data.csv", 
                                   show_col_types = FALSE)
trinary_classifier_data <- read_csv("trinary-classifier-data.csv", 
                                    show_col_types = FALSE)

# Displaying the first few rows of each dataset
head(binary_classifier_data)
head(trinary_classifier_data)
```


Binary classification dataset.

```{r, echo=TRUE, results='markup'}
# Plotting the binary classification dataset using a scatter plot.
ggplot(binary_classifier_data, aes(x = x, y = y, color = as.factor(label))) +
  geom_point() +
  labs(title = "Binary Classification Data", color = "Label") +
  theme_minimal()
```


Trinary classification datasets.

```{r, echo=TRUE, results='markup'}
# Plotting the trinary classification dataset using a scatter plot.
ggplot(trinary_classifier_data, aes(x = x, y = y, color = as.factor(label))) +
  geom_point() +
  labs(title = "Trinary Classification Data", color = "Label") +
  theme_minimal()
```



ii. Determine which points are nearest by calculating the Euclidean distance 
    between two points.
    
    
    
```{r, echo=TRUE, results='markup'}
# Function calculating the Euclidean distance between two points
euclidean_distance <- function(point1, point2) {
  sqrt(sum((point1 - point2)^2))
}

# Example usage
point_1 <- c(1, 2)
point_2 <- c(4, 6)
distance <- euclidean_distance(point_1, point_2)
print(paste("Euclidean Distance:", distance))
```


Binary Classification and model fitting. 

Splitting binary dataset into training and test sets, and fitting a k-nearest 
neighbors model. 

```{r, echo=TRUE, results='markup'}
set.seed(123)
train_indices <- sample(seq_len(nrow(binary_classifier_data)), size = 0.7 * 
                            nrow(binary_classifier_data))
train_binary <- binary_classifier_data[train_indices, ]
test_binary <- binary_classifier_data[-train_indices, ]

# Apply k-NN algorithm
k <- 3
predicted_binary <- knn(train_binary[, c("x", "y")], test_binary[, c("x", "y")], 
                        train_binary$label, k)

# Evaluate model performance
confusionMatrix(predicted_binary, as.factor(test_binary$label))
```


Trinary Classification and model fitting.

Model Fitting

Splitting the trinary dataset into training and test tests, and fitting a 
k-nearest neighbors model.

```{r, echo=TRUE, results='markup'}
set.seed(123)
train_indices <- sample(seq_len(nrow(trinary_classifier_data)), size = 0.7 * 
                            nrow(trinary_classifier_data))
train_trinary <- trinary_classifier_data[train_indices, ]
test_trinary <- trinary_classifier_data[-train_indices, ]

# Apply k-NN algorithm
predicted_trinary <- knn(train_trinary[, c("x", "y")], 
                         test_trinary[, c("x", "y")], train_trinary$label, k)

# Evaluate model performance
confusionMatrix(predicted_trinary, as.factor(test_trinary$label))

```


Accuracy

Calculating Accuracy for Binary Classification

```{r, echo=TRUE, results='markup'}
# Calculate accuracy for binary classification
conf_matrix_binary <- confusionMatrix(predicted_binary, 
                                      as.factor(test_binary$label))
accuracy_binary <- conf_matrix_binary$overall['Accuracy']
print(paste("Accuracy for Binary Classification:", accuracy_binary))


```

Calculating Accuracy for Trinary Classification

```{r, echo=TRUE, results='markup'}
# Calculate accuracy for trinary classification
conf_matrix_trinary <- confusionMatrix(predicted_trinary, 
                                       as.factor(test_trinary$label))
accuracy_trinary <- conf_matrix_trinary$overall['Accuracy']
print(paste("Accuracy for Trinary Classification:", accuracy_trinary))

```
ii. Fit a k nearest neighborsâ€™ model for each dataset for k=3, k=5, k=10, k=15, 
k=20, and k=25. Compute the accuracy of the resulting models for each value of k. 
Plot the results in a graph where the x-axis is the different values of k and 
the y-axis is the accuracy of the model.

Defining the values for k

```{r, echo=TRUE, results='markup'}

k_values <- c(3, 5, 10, 15, 20, 25)

```


Computing the accuracy for binary classification

```{r, echo=TRUE, results='markup'}
# Splitting binary dataset into training and test sets
set.seed(123)
train_indices <- sample(seq_len(nrow(binary_classifier_data)), 
                        size = 0.7 * nrow(binary_classifier_data))
train_binary <- binary_classifier_data[train_indices, ]
test_binary <- binary_classifier_data[-train_indices, ]

# Calculate accuracies for different values of k
binary_accuracies <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  k <- k_values[i]
  predicted_binary <- knn(train_binary[, c("x", "y")], 
                          test_binary[, c("x", "y")], train_binary$label, k)
  conf_matrix_binary <- confusionMatrix(predicted_binary,
                                        as.factor(test_binary$label))
  binary_accuracies[i] <- conf_matrix_binary$overall['Accuracy']
}

binary_accuracies

```

Computing the accuracy for trinary classification

```{r, echo=TRUE, results='markup'}
# Splitting trinary dataset into training and test sets
set.seed(123)
train_indices <- sample(seq_len(nrow(trinary_classifier_data)), 
                        size = 0.7 * nrow(trinary_classifier_data))
train_trinary <- trinary_classifier_data[train_indices, ]
test_trinary <- trinary_classifier_data[-train_indices, ]

# Calculate accuracies for different values of k
trinary_accuracies <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  k <- k_values[i]
  predicted_trinary <- knn(train_trinary[, c("x", "y")], 
                           test_trinary[, c("x", "y")], train_trinary$label, k)
  conf_matrix_trinary <- confusionMatrix(predicted_trinary, 
                                         as.factor(test_trinary$label))
  trinary_accuracies[i] <- conf_matrix_trinary$overall['Accuracy']
}

trinary_accuracies


```

Plotting the results for both classifications.

```{r, echo=TRUE, results='markup'}
# Creating a data frame for plotting
accuracy_data <- data.frame(
  k = rep(k_values, 2),
  accuracy = c(binary_accuracies, trinary_accuracies),
  dataset = rep(c("Binary", "Trinary"), each = length(k_values))
)

# Plot the results
ggplot(accuracy_data, aes(x = k, y = accuracy, color = dataset, 
                          group = dataset)) +
  geom_line() +
  geom_point() +
  labs(title = "k-NN Accuracy for Different Values of k",
       x = "k",
       y = "Accuracy",
       color = "Dataset") +
  theme_minimal()
```

i. Looking back at the plots of the data, do you think a linear classifier would
   work well on these datasets?

Answer:

Looking at the Binary Classification Dataset, we can clearly see that both 0 
and 1 labels are almost distinctly separated from each other in different 
clusters. The points on the graph for each label are almost clearly separated. 
This suggests that a linear classifier would likely work well on this dataset.

Looking at the Trinary Classification Dataset scatter plot, the three different
clusters for the labels 0, 1, and 2 are shown to be less separated. Many points 
for different labels overlap with each other. This indicates that a linear 
classifier might not be a good model to use because the accuracy would probably 
be low.

ii. How does the accuracy of your logistic regression classifier from last week 
    compare? Why is the accuracy different between these two methods?
    
Answer:

The logistic regression classifier model from last week for the Thoracic Surgery
Binary Dataset had an accuracy rate of 82.98%, indicating that a linear model 
works well for this dataset.

However, the logistic regression model's accuracy rate for the binary 
classification dataset is much lower at 53.51%. This is significantly lower than
the k-NN models' accuracy rates, which range from 97.11% to 98.22%. This 
suggests that k-NN models have a greater ability to capture non-linear 
relationships, leading to much better performance on this dataset.



2. Clustering


i. Plot the dataset using a scatter plot.

Loading the data from the dataset.

```{r, echo=TRUE, results='markup'}
# Loading necessary libraries
library(ggplot2)

# Loading the dataset
data <- read.csv("C:/Users/maxim/OneDrive/Desktop/BU/DSC 520/clustering-data.csv")

```

Exploring the data in the dataset to get a better insight.

```{r, echo=TRUE, results='markup'}
# Explore the data
str(data)
summary(data)

```

Plotting the data from the dataset using a scatter plot.

```{r, echo=TRUE, results='markup'}
# Plotting the data from the dataset using a scatter plot
ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  labs(title = "Scatter Plot of Clustering Data", x = "X Coordinate", 
       y = "Y Coordinate") +
  theme_minimal()
```


ii. Fit the dataset using the k-means algorithm from k=2 to k=12. Create a 
    scatter plot of the resultant clusters for each value of k.

```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Loading necessary libraries
library(ggplot2)
library(dplyr)

# Loading the data from the dataset
data <- read.csv("C:/Users/maxim/OneDrive/Desktop/BU/DSC 520/clustering-data.csv")

# Function to plot k-means clusters
plot_kmeans <- function(data, k) {
  set.seed(123)  # For reproducibility
  kmeans_result <- kmeans(data, centers = k)
  data$cluster <- as.factor(kmeans_result$cluster)
  
  ggplot(data, aes(x = x, y = y, color = cluster)) +
    geom_point() +
    labs(title = paste("k-means Clustering with k =", k), x = "X Coordinate", 
         y = "Y Coordinate") +
    theme_minimal() +
    scale_color_discrete(name = "Cluster")
}

# Looping through k values from 2 to 12 and create plots
plots <- lapply(2:12, function(k) plot_kmeans(data, k))

# Displaying the plots
for (plot in plots) {
  print(plot)
}

```


iii. Compute the distance of each data point to the center of the cluster it is 
     assigned to and take the average value of all of those distances.
     
```{r, echo=TRUE, warning=FALSE, message=FALSE}

# Computing averages of distance to the cluster centers
compute_avg_distance <- function(data, k) {
  set.seed(123) 
  kmeans_result <- kmeans(data, centers = k)
  centers <- kmeans_result$centers
  clusters <- kmeans_result$cluster
  distances <- sqrt(rowSums((data - centers[clusters, ])^2))
  avg_distance <- mean(distances)
  return(avg_distance)
}

# Computing and printing average distances for k values from 2 to 12
avg_distances <- sapply(2:12, function(k) compute_avg_distance(data, k))
avg_distances_df <- data.frame(k = 2:12, avg_distance = avg_distances)

# Printing the results
avg_distances_df

```

e. Calculate this average distance from the center of each cluster for each 
   value of k and plot it as a line chart where k is the x-axis and the average 
   distance is the y-axis.
   
```{r, echo=TRUE, warning=FALSE, message=FALSE}
# Computing average distance to cluster centers
compute_avg_distance <- function(data, k) {
  set.seed(123)  # For reproducibility
  kmeans_result <- kmeans(data, centers = k)
  centers <- kmeans_result$centers
  clusters <- kmeans_result$cluster
  distances <- sqrt(rowSums((data - centers[clusters, ])^2))
  avg_distance <- mean(distances)
  return(avg_distance)
}

# Computing and printing average distances for k values from 2 to 12
avg_distances <- sapply(2:12, function(k) compute_avg_distance(data, k))
avg_distances_df <- data.frame(k = 2:12, avg_distance = avg_distances)

# Printing the results
avg_distances_df

# Plotting the average distance for each k value
ggplot(avg_distances_df, aes(x = k, y = avg_distance)) +
  geom_line() +
  geom_point() +
  labs(title = "Average Distance to Cluster Centers for Different k Values",
       x = "Number of Clusters (k)", y = "Average Distance to Center") +
  theme_minimal()
```


f. Looking at the graph you generated in the previous example, what is the elbow 
   point for this dataset?

Answer:

Based on the generated graph in the previous example, the elbow point for this 
dataset appears at k=5. This is the point where the decrease in average distance
significantly slows down, creating the elbow point. Additionally, we can state 
that the optimal number of clusters is likely k=5. Therefore, 5 clusters 
probably represent the optimal number for this dataset.